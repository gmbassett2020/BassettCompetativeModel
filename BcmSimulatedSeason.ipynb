{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce the simply synthetic forecast from the Java Neural Network using the sklearn.neural_network.\n",
    "# https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    "# 2021-09-04\n",
    "\n",
    "# *********************************************************\n",
    "# NOTE: This is deprecated.  See simulatedFootball instead.\n",
    "# *********************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "#\n",
    "# Some notes:\n",
    "#\n",
    "# random_stateint, RandomState instance, default=None\n",
    "# Determines random number generation for weights and bias initialization, train-test split if early stopping\n",
    "# is used, and batch sampling when solver=’sgd’ or ‘adam’. Pass an int for reproducible results across multiple\n",
    "# function calls. See Glossary.\n",
    "#\n",
    "# activation : {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default=’relu’\n",
    "# Activation function for the hidden layer.\n",
    "#    ‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x\n",
    "#    ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n",
    "#    ‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).\n",
    "#    ‘relu’, the rectified linear unit function, returns f(x) = max(0, x)\n",
    "#\n",
    "# verbose : bool, default=False\n",
    "# Whether to print progress messages to stdout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation data.\n",
    "import random\n",
    "import numpy.random\n",
    "from numpy.random import default_rng\n",
    "dataSetSize = 2500\n",
    "numBins = 20\n",
    "binSize = 1./numBins\n",
    "randomSeed = 20210905\n",
    "random.seed(randomSeed)\n",
    "default_rng(randomSeed)\n",
    "allowContinuousOutputValues = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying this to simulatedSeason:\n",
    "\n",
    "# What does a typical season of NCAA Div 1 look like?\n",
    "#\n",
    "# 2017 colI_2017wk00out totals: all  16   conf   0   nonConf  14   nonDiv   2\n",
    "# 2017 colI_2017wk01out totals: all 216   conf   8   nonConf 116   nonDiv  92\n",
    "# 2017 colI_2017wk02out totals: all 218   conf  24   nonConf 144   nonDiv  50\n",
    "# 2017 colI_2017wk03out totals: all 220   conf  34   nonConf 158   nonDiv  28\n",
    "# 2017 colI_2017wk04out totals: all 218   conf 142   nonConf  70   nonDiv   6\n",
    "# 2017 colI_2017wk05out totals: all 214   conf 166   nonConf  46   nonDiv   2\n",
    "# 2017 colI_2017wk06out totals: all 222   conf 194   nonConf  26   nonDiv   2\n",
    "# 2017 colI_2017wk07out totals: all 228   conf 212   nonConf  16   nonDiv   0\n",
    "# 2017 colI_2017wk08out totals: all 220   conf 206   nonConf  14   nonDiv   0\n",
    "# 2017 colI_2017wk09out totals: all 226   conf 216   nonConf   8   nonDiv   2\n",
    "# 2017 colI_2017wk10out totals: all 242   conf 222   nonConf  20   nonDiv   0\n",
    "# 2017 colI_2017wk11out totals: all 236   conf 222   nonConf  12   nonDiv   2\n",
    "# 2017 colI_2017wk12out totals: all 240   conf 214   nonConf  16   nonDiv  10\n",
    "# 2017 colI_2017wk13out totals: all 144   conf 116   nonConf  28   nonDiv   0\n",
    "# 2017 colI_2017wk14out totals: all  50   conf  38   nonConf  12   nonDiv   0\n",
    "# 2017 colI_2017wk15out totals: all  10   conf   0   nonConf  10   nonDiv   0\n",
    "# 2017 colI_2017wk16out totals: all  82   conf   0   nonConf  82   nonDiv   0\n",
    "# 2017 colI_2017wk17out totals: all   4   conf   2   nonConf   2   nonDiv   0\n",
    "#\n",
    "# defAve, offAve - average for that divsion\n",
    "# defRms, offRms - variation of the def & off inside that division\n",
    "# defChange, offChange - RMS change for off & def between years\n",
    "#\n",
    "# Division IA  defAve  5.43 offAve  7.88 defRms 0.40 offRms 0.48 defChange 0.46 offChange 0.57\n",
    "# Division IAA defAve  4.88 offAve  6.97 defRms 0.41 offRms 0.48 defChange 0.47 offChange 0.54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "Independent\n"
     ]
    }
   ],
   "source": [
    "# Copying this to simulatedSeason:\n",
    "\n",
    "class Team:\n",
    "    nextTeamId = 0\n",
    "    \n",
    "    def __init__(self, teamName):\n",
    "        self.name = teamName\n",
    "        self.teamId = Team.nextTeamId\n",
    "        Team.nextTeamId += 1\n",
    "        self.conference = {}\n",
    "        self.division = {}\n",
    "    \n",
    "    def set_conference(self, year, conference, division):\n",
    "        self.conference[year] = conference\n",
    "        self.division[year] = division\n",
    "\n",
    "print(Team.nextTeamId)\n",
    "sampleTeam = Team('BYU')\n",
    "sampleTeam.set_conference(2021, \"Independent\", \"IA\")\n",
    "print(Team.nextTeamId)\n",
    "print(sampleTeam.conference[2021])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and validation arrays, filling in with random values which will then be scaled/adjusted.\n",
    "#X_train = np.empty(dataSetSize,dtype=float)\n",
    "#X_valid = np.empty(dataSetSize,dtype=float)\n",
    "#y_train = np.empty([dataSetSize,numBins],dtype=float)\n",
    "#y_valid = np.empty([dataSetSize,numBins],dtype=float)\n",
    "X_train = default_rng().random((dataSetSize,1))\n",
    "X_valid = default_rng().random((dataSetSize,1))\n",
    "result_train = default_rng().random((dataSetSize))\n",
    "result_valid = default_rng().random((dataSetSize))\n",
    "y_train = np.empty((dataSetSize,numBins))\n",
    "y_valid = np.empty((dataSetSize,numBins))\n",
    "y_probs = np.arange(start=0.0, stop=1.0, step=binSize)\n",
    "#print (y_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale X data so that the forecast is for the favored team (i.e. preditions range from 0.5 to 1)\n",
    "X_train = 0.5*X_train + 0.5\n",
    "X_valid = 0.5*X_valid + 0.5\n",
    "# Scale the data for tanh.  Is this needed?\n",
    "#X_train = 2.*(X_train - 0.5)\n",
    "#X_valid = 2.*(X_valid - 0.5)\n",
    "#result_train = 2.*(result_train - 0.5)\n",
    "#result_valid = 2.*(result_train - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish setting the result, which is expected+pertubation.\n",
    "result_train = result_train + X_train[:,0] - 0.5\n",
    "result_valid = result_valid + X_valid[:,0] - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "[0.13792537 1.13022876 0.56389748 0.66494079 0.45610057 0.78953954\n",
      " 0.81884201 0.8161164  0.53632294 0.99445029]\n",
      "[[1.         1.         0.75850738 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         0.27794963\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         0.29881577 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         0.12201146 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         0.79079085 0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         0.37684023 0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         0.32232805 0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         0.72645879 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         0.88900578]]\n"
     ]
    }
   ],
   "source": [
    "nonBinaryLabelsAllowed = allowContinuousOutputValues # Multioutput target data is not supported with label binarization\n",
    "\n",
    "# Fill in y according to result > 0\n",
    "for index in range(dataSetSize):\n",
    "    for pIndex in range(numBins):\n",
    "        if (result_train[index] >= y_probs[pIndex]):\n",
    "            y_train[index,pIndex] = 1.0\n",
    "        else:\n",
    "            #y_train[index,pIndex] = -1.0\n",
    "            y_train[index,pIndex] = 0.0\n",
    "        if (result_valid[index] >= y_probs[pIndex]):\n",
    "            y_valid[index,pIndex] = 1.0\n",
    "        else:\n",
    "            #y_valid[index,pIndex] = -1.0\n",
    "            y_valid[index,pIndex] = 0.0\n",
    "    # Set the value at the transition point\n",
    "    if (nonBinaryLabelsAllowed):\n",
    "        iResid = int(result_train[index]/binSize)\n",
    "        if (iResid >= 0 and iResid < numBins):\n",
    "            y_train[index,iResid] = result_train[index]/binSize - iResid\n",
    "        iResid = int(result_valid[index]/binSize)\n",
    "        if (iResid >= 0 and iResid < numBins):\n",
    "            y_valid[index,iResid] = result_valid[index]/binSize - iResid\n",
    "print(dataSetSize)        \n",
    "print(result_train[0:10])\n",
    "print(y_train[0:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.41324839\n",
      "Iteration 2, loss = 0.29338288\n",
      "Iteration 3, loss = 0.21265713\n",
      "Iteration 4, loss = 0.15494356\n",
      "Iteration 5, loss = 0.11422268\n",
      "Iteration 6, loss = 0.08878514\n",
      "Iteration 7, loss = 0.07601225\n",
      "Iteration 8, loss = 0.07113037\n",
      "Iteration 9, loss = 0.06950419\n",
      "Iteration 10, loss = 0.06890478\n",
      "Iteration 11, loss = 0.06859408\n",
      "Iteration 12, loss = 0.06844237\n",
      "Iteration 13, loss = 0.06820568\n",
      "Iteration 14, loss = 0.06803529\n",
      "Iteration 15, loss = 0.06789880\n",
      "Iteration 16, loss = 0.06773212\n",
      "Iteration 17, loss = 0.06758058\n",
      "Iteration 18, loss = 0.06741342\n",
      "Iteration 19, loss = 0.06728463\n",
      "Iteration 20, loss = 0.06712842\n",
      "Iteration 21, loss = 0.06701858\n",
      "Iteration 22, loss = 0.06683567\n",
      "Iteration 23, loss = 0.06674987\n",
      "Iteration 24, loss = 0.06658949\n",
      "Iteration 25, loss = 0.06647015\n",
      "Iteration 26, loss = 0.06638356\n",
      "Iteration 27, loss = 0.06626365\n",
      "Iteration 28, loss = 0.06613180\n",
      "Iteration 29, loss = 0.06605246\n",
      "Iteration 30, loss = 0.06591866\n",
      "Iteration 31, loss = 0.06586813\n",
      "Iteration 32, loss = 0.06578266\n",
      "Iteration 33, loss = 0.06567685\n",
      "Iteration 34, loss = 0.06566741\n",
      "Iteration 35, loss = 0.06550730\n",
      "Iteration 36, loss = 0.06544545\n",
      "Iteration 37, loss = 0.06539715\n",
      "Iteration 38, loss = 0.06532351\n",
      "Iteration 39, loss = 0.06528283\n",
      "Iteration 40, loss = 0.06520454\n",
      "Iteration 41, loss = 0.06514700\n",
      "Iteration 42, loss = 0.06512771\n",
      "Iteration 43, loss = 0.06515035\n",
      "Iteration 44, loss = 0.06511085\n",
      "Iteration 45, loss = 0.06497484\n",
      "Iteration 46, loss = 0.06493533\n",
      "Iteration 47, loss = 0.06489158\n",
      "Iteration 48, loss = 0.06489175\n",
      "Iteration 49, loss = 0.06492179\n",
      "Iteration 50, loss = 0.06488722\n",
      "Iteration 51, loss = 0.06478201\n",
      "Iteration 52, loss = 0.06482326\n",
      "Iteration 53, loss = 0.06478550\n",
      "Iteration 54, loss = 0.06475791\n",
      "Iteration 55, loss = 0.06469002\n",
      "Iteration 56, loss = 0.06476389\n",
      "Iteration 57, loss = 0.06473007\n",
      "Iteration 58, loss = 0.06466194\n",
      "Iteration 59, loss = 0.06460144\n",
      "Iteration 60, loss = 0.06460902\n",
      "Iteration 61, loss = 0.06460985\n",
      "Iteration 62, loss = 0.06457250\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='tanh', hidden_layer_sizes=(20, 20), max_iter=5000,\n",
       "             verbose=True)"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train the model.\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "if (allowContinuousOutputValues):\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=(numBins,numBins),activation='tanh',verbose=True,max_iter=5000)\n",
    "else:\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(numBins,numBins),activation='tanh',verbose=True,max_iter=5000)\n",
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "[0.59699911 0.99916623 0.61723723 0.94726927 0.79059156 0.81991023\n",
      " 0.97335378 0.58718801 0.51123589 0.55500898 0.74489005 0.7623809\n",
      " 0.7644073  0.61709111 0.92472435 0.73925784 0.77188456 0.77062802\n",
      " 0.99062909 0.78333335 0.92786468 0.60331027 0.58971355 0.6151738\n",
      " 0.90668706 0.57310792 0.82353137 0.55538432 0.82337144 0.50439528\n",
      " 0.68316438 0.62029158 0.58275025 0.9706761  0.96791077 0.99050878\n",
      " 0.74675999 0.93762277 0.55794857 0.9711452  0.96739598 0.75800253\n",
      " 0.88160878 0.7919184  0.67467566 0.5745779  0.51181457 0.82494955\n",
      " 0.80999431 0.94172392]\n",
      "pred:\n",
      "[[0.96061423 0.9645793  0.94028123 0.92725219 0.86790464 0.84321862\n",
      "  0.78760538 0.73503064 0.6776007  0.6195251  0.59492387 0.56472848\n",
      "  0.48229037 0.44004305 0.3876153  0.33502093 0.27988697 0.22237746\n",
      "  0.17517726 0.1232447 ]\n",
      " [1.04244961 1.03030844 1.0392549  1.01992736 1.05565054 1.06228201\n",
      "  1.06046946 1.0345783  1.01623269 1.0390711  0.93249842 0.86169738\n",
      "  0.85362007 0.83012599 0.77274864 0.7604356  0.71548457 0.65526868\n",
      "  0.58356031 0.53115073]\n",
      " [0.9667101  0.96892407 0.94790184 0.93490973 0.87947048 0.85580801\n",
      "  0.80243672 0.75305268 0.69852974 0.64345053 0.61377637 0.58200778\n",
      "  0.5023934  0.46140569 0.41083817 0.35885716 0.30277157 0.24674791\n",
      "  0.1978759  0.14486261]\n",
      " [1.03649926 1.02333074 1.03276379 1.01474576 1.03712559 1.03746304\n",
      "  1.02805847 1.00371825 0.98198296 0.99294238 0.89377158 0.82991106\n",
      "  0.81014792 0.78492483 0.7334339  0.7123693  0.6621341  0.60733182\n",
      "  0.53779361 0.48197344]\n",
      " [1.01040298 1.00041815 1.00194694 0.98734123 0.97038446 0.9565499\n",
      "  0.92498672 0.89593685 0.8615143  0.83815225 0.76776964 0.7213777\n",
      "  0.66989483 0.63830192 0.59505996 0.55403489 0.49535488 0.44679892\n",
      "  0.38550838 0.32648874]\n",
      " [1.01624672 1.00498904 1.00902324 0.99387611 0.98407782 0.97239391\n",
      "  0.9448557  0.91775309 0.88602973 0.86881986 0.79235647 0.7430747\n",
      "  0.69708482 0.66682933 0.62315131 0.58510782 0.52716403 0.47853309\n",
      "  0.41549085 0.35632105]\n",
      " [1.03965021 1.02686494 1.03625011 1.01757907 1.04666169 1.05005778\n",
      "  1.04445882 1.01952973 0.99954182 1.01645532 0.91342755 0.84615108\n",
      "  0.83219611 0.80786284 0.7536026  0.73680908 0.68907557 0.6317659\n",
      "  0.56110503 0.50684535]\n",
      " [0.95758726 0.96240994 0.93649333 0.92342659 0.8622388  0.83705011\n",
      "  0.78037929 0.72621086 0.6673223  0.60786085 0.58572526 0.5562947\n",
      "  0.47251873 0.42964935 0.37625353 0.32340463 0.26877011 0.21051592\n",
      "  0.16414147 0.11274703]\n",
      " [0.93262057 0.94406582 0.90517984 0.89136268 0.81723926 0.78781557\n",
      "  0.72368691 0.65634716 0.58499356 0.51643692 0.51328612 0.48999689\n",
      "  0.39648063 0.34856142 0.28634484 0.23236636 0.18229568 0.1180236\n",
      "  0.0783498  0.03126651]\n",
      " [0.94733633 0.9549866  0.92364991 0.91036592 0.84340538 0.81651408\n",
      "  0.75651865 0.69693237 0.6330233  0.56934179 0.55529334 0.52840175\n",
      "  0.44037211 0.39541218 0.33855386 0.28505378 0.23221477 0.17144075\n",
      "  0.12784138 0.07825634]\n",
      " [1.00039652 0.99293583 0.98970892 0.97581781 0.94800008 0.9311827\n",
      "  0.89350239 0.86048838 0.82150084 0.7889147  0.72856364 0.68637775\n",
      "  0.62676014 0.59293867 0.54937269 0.50438095 0.44528413 0.39595629\n",
      "  0.33759756 0.27939953]\n",
      " [1.00435542 0.99585551 0.99456592 0.98042163 0.95671287 0.94098875\n",
      "  0.90562482 0.87425826 0.83707207 0.80795813 0.74369295 0.69993731\n",
      "  0.64337117 0.61042356 0.56712446 0.52355411 0.46451438 0.41560349\n",
      "  0.35609353 0.29750158]\n",
      " [1.00480372 0.99618911 0.99511476 0.98093945 0.95771074 0.94211697\n",
      "  0.90702347 0.87583747 0.83885552 0.8101487  0.74543586 0.70149528\n",
      "  0.64528763 0.61243958 0.56916001 0.52576204 0.46673701 0.41786494\n",
      "  0.35822395 0.29959254]\n",
      " [0.9666668  0.9688933  0.94784776 0.93485558 0.87938758 0.85571776\n",
      "  0.80233    0.75292342 0.69837997 0.64327849 0.61364086 0.58188362\n",
      "  0.50224854 0.46125185 0.41067158 0.35868569 0.30260658 0.24657247\n",
      "  0.19771238 0.14470672]\n",
      " [1.03350928 1.02022608 1.02937998 1.01191429 1.02851655 1.02637983\n",
      "  1.01370588 0.98956    0.96623951 0.97208865 0.87647344 0.8154462\n",
      "  0.79077551 0.76474742 0.71533538 0.69079219 0.63864791 0.58566558\n",
      "  0.51715182 0.46023053]\n",
      " [0.99908765 0.99197971 0.98809938 0.97428404 0.94515696 0.92799904\n",
      "  0.88957991 0.85600195 0.81641935 0.78273177 0.7236596  0.68196943\n",
      "  0.62138556 0.58727713 0.54358799 0.49816367 0.43907478 0.38958226\n",
      "  0.33160197 0.27355046]\n",
      " [1.0064393  0.99741197 0.997115   0.9828223  0.96137181 0.94626596\n",
      "  0.9121739  0.88163537 0.84539925 0.81820304 0.75184918 0.70722038\n",
      "  0.65234444 0.61986072 0.57663266 0.53388467 0.4749287  0.42618233\n",
      "  0.36606203 0.30729647]\n",
      " [1.00616649 0.99720735 0.99678161 0.98250896 0.96075889 0.94557027\n",
      "  0.91130953 0.88066427 0.84430367 0.8168527  0.75077341 0.70626091\n",
      "  0.65116019 0.61861558 0.57538113 0.53252237 0.47355317 0.42478762\n",
      "  0.3647474  0.30600311]\n",
      " [1.04155835 1.02917505 1.03830958 1.01920003 1.05272679 1.05826519\n",
      "  1.05519831 1.0296667  1.0107872  1.03166305 0.92623291 0.85661333\n",
      "  0.84657822 0.82281098 0.76650548 0.75268307 0.7067782  0.64757024\n",
      "  0.57620151 0.52314622]\n",
      " [1.00888686 0.99926055 1.00010145 0.98562036 0.96691271 0.95257605\n",
      "  0.92002782 0.89042281 0.85530534 0.83044665 0.76161387 0.7159132\n",
      "  0.66310345 0.6311684  0.58795614 0.5462461  0.48744137 0.43883293\n",
      "  0.37799154 0.31905636]\n",
      " [1.03394076 1.02066154 1.02987211 1.01233028 1.0297361  1.02793468\n",
      "  1.01571491 0.99155956 0.96846417 0.97502282 0.87890006 0.81748479\n",
      "  0.79349124 0.76757735 0.71789341 0.69382278 0.64193031 0.5887137\n",
      "  0.52005416 0.46327262]\n",
      " [0.96253669 0.96595269 0.94268572 0.92967405 0.87152929 0.84716398\n",
      "  0.79224132 0.74067596 0.6841671  0.62700634 0.60082076 0.57013447\n",
      "  0.48856755 0.44671647 0.39488884 0.34247278 0.28703041 0.22999209\n",
      "  0.18226593 0.12999176]\n",
      " [0.9583709  0.96297242 0.93747419 0.9244184  0.86370084 0.8386421\n",
      "  0.78224162 0.72848622 0.66997628 0.61086728 0.58809677 0.55846908\n",
      "  0.4750356  0.43232705 0.37918446 0.32639849 0.27163313 0.21357195\n",
      "  0.166984   0.11545028]\n",
      " [0.96609778 0.96848887 0.94713687 0.93414347 0.87829899 0.85453269\n",
      "  0.80092921 0.75122612 0.69641293 0.64101997 0.61186187 0.58025366\n",
      "  0.50034733 0.45923268 0.40848411 0.35643497 0.30044135 0.24426972\n",
      "  0.19556618 0.142661  ]\n",
      " [1.03093601 1.01770464 1.02642211 1.00938794 1.0213843  1.01737952\n",
      "  1.00210516 0.97790306 0.9532617  0.95505203 0.86242865 0.8035886\n",
      "  0.77506998 0.74837267 0.70041068 0.67322906 0.61972691 0.56797046\n",
      "  0.50031371 0.44267422]\n",
      " [0.95316235 0.95922132 0.93095209 0.91780813 0.85404388 0.82812216\n",
      "  0.76996873 0.71346389 0.65242389 0.59105317 0.57245828 0.54413139\n",
      "  0.45846948 0.41469469 0.35983682 0.30666928 0.25279199 0.1934473\n",
      "  0.14827478 0.09766517]\n",
      " [1.01693735 1.00554262 1.00985513 0.99463718 0.9857318  0.97432785\n",
      "  0.94729156 0.92039616 0.88899462 0.87255561 0.79536184 0.74571188\n",
      "  0.70041516 0.67031996 0.62655274 0.58890154 0.53107489 0.48240186\n",
      "  0.41915017 0.35998385]\n",
      " [0.94745871 0.95507602 0.92380336 0.91052275 0.84362714 0.81675632\n",
      "  0.75679834 0.69727679 0.63342843 0.56979315 0.55565055 0.52872895\n",
      "  0.44074779 0.39581267 0.33899718 0.28550315 0.23264191 0.17189776\n",
      "  0.12826545 0.07865904]\n",
      " [1.016907   1.00551822 1.00981858 0.99460378 0.98565893 0.97424255\n",
      "  0.94718406 0.92027967 0.88886397 0.87239086 0.79522926 0.74559561\n",
      "  0.70026821 0.67016595 0.62640283 0.58873419 0.53090225 0.48223123\n",
      "  0.41898876 0.35982218]\n",
      " [0.93024305 0.94226703 0.90219292 0.88826718 0.8131002  0.78324977\n",
      "  0.71851927 0.64993684 0.57735256 0.50813057 0.50666121 0.48395816\n",
      "  0.38961223 0.34121947 0.27810422 0.22408876 0.17447831 0.10966451\n",
      "  0.07061832 0.02392346]\n",
      " [0.98515084 0.98198241 0.97087082 0.95764082 0.91588896 0.89558742\n",
      "  0.8500224  0.80998415 0.76404621 0.71988835 0.67398632 0.63701277\n",
      "  0.56723607 0.53012132 0.48424179 0.43515711 0.37681118 0.32494678\n",
      "  0.27094014 0.21481388]\n",
      " [0.9676126  0.96956508 0.94902912 0.9360379  0.88120123 0.85769234\n",
      "  0.80466621 0.75575175 0.7016559  0.64704448 0.61660703 0.58460098\n",
      "  0.50542045 0.46462003 0.41431684 0.36243905 0.30621952 0.25041328\n",
      "  0.20129274 0.14812034]\n",
      " [0.95620286 0.96141467 0.93476015 0.92167204 0.85966389 0.83424583\n",
      "  0.77710317 0.72220445 0.66264526 0.60257179 0.58155208 0.55246846\n",
      "  0.4680938  0.42494066 0.37109316 0.31813799 0.26373712 0.20514174\n",
      "  0.159144   0.10799547]\n",
      " [1.0393418  1.02650478 1.0359132  1.01730976 1.04570375 1.04877611\n",
      "  1.04278547 1.01793468 0.99777154 1.0140721  0.91142754 0.84450852\n",
      "  0.82995109 0.80552846 0.75157027 0.73432631 0.68632158 0.62928919\n",
      "  0.55874055 0.50430641]\n",
      " [1.0390197  1.02613222 1.03556025 1.01702649 1.04470941 1.04744979\n",
      "  1.04105488 1.01628069 0.9959356  1.01160355 0.90935779 0.84280629\n",
      "  0.82762816 0.80311277 0.74946229 0.73175599 0.68347462 0.62672381\n",
      "  0.55629179 0.50168092]\n",
      " [1.04154554 1.02915904 1.03829592 1.01918944 1.05268524 1.0582084\n",
      "  1.05512386 1.02959702 1.01070994 1.03155815 0.92614432 0.85654128\n",
      "  0.84647868 0.82270755 0.76641687 0.75257339 0.70665531 0.64746122\n",
      "  0.57609733 0.52303318]\n",
      " [1.00082741 0.99325151 0.9902384  0.9763215  0.94894001 0.93223689\n",
      "  0.89480265 0.86197231 0.82318069 0.79096207 0.73018836 0.68783686\n",
      "  0.62854183 0.59481503 0.55128595 0.5064405  0.44734387 0.39806748\n",
      "  0.33958395 0.28133934]\n",
      " [1.03525046 1.02200836 1.03135829 1.01357817 1.03348358 1.03274329\n",
      "  1.02193739 0.99771621 0.97531137 0.98407963 0.88640507 0.82377039\n",
      "  0.80189434 0.776331   0.7257656  0.70318817 0.65210732 0.59812305\n",
      "  0.52901692 0.47269772]\n",
      " [0.94829305 0.95568509 0.92484948 0.91159139 0.84514094 0.81840959\n",
      "  0.7587083  0.69962809 0.63619308 0.57287561 0.55808959 0.53096327\n",
      "  0.44331401 0.39854816 0.34202374 0.28857198 0.23555965 0.17501934\n",
      "  0.13116237 0.08141003]\n",
      " [1.03939608 1.02656792 1.03597257 1.01735729 1.04587191 1.04900083\n",
      "  1.04307879 1.01821458 0.9980822  1.01449012 0.91177821 0.84479668\n",
      "  0.83034469 0.80593775 0.75192694 0.73476169 0.68680424 0.6297236\n",
      "  0.55915525 0.50475144]\n",
      " [1.03895933 1.02606279 1.03549398 1.01697318 1.04452374 1.04720259\n",
      "  1.04073244 1.01597203 0.99559296 1.01114318 0.90897201 0.84248874\n",
      "  0.82719523 0.80266253 0.74906884 0.73127681 0.68294432 0.6262454\n",
      "  0.55583517 0.50119178]\n",
      " [1.00337944 0.99513149 0.99337021 0.97929177 0.95454856 0.93854549\n",
      "  0.90259869 0.87083465 0.83320404 0.8032138  0.73992005 0.69656179\n",
      "  0.6392246  0.60606069 0.56271127 0.51877397 0.45970823 0.41070665\n",
      "  0.35148147 0.29297924]\n",
      " [1.0270867  1.01413677 1.0219355  1.00548241 1.01110865 1.00466744\n",
      "  0.9858034  0.96120947 0.93464973 0.9308499  0.8426007  0.78668491\n",
      "  0.75293654 0.72526991 0.67900567 0.64837208 0.59323221 0.54284422\n",
      "  0.47643601 0.41803331]\n",
      " [1.01067715 1.00062861 1.00228029 0.98765137 0.97101564 0.95727412\n",
      "  0.92589152 0.89694004 0.86264334 0.83955608 0.76889201 0.7223727\n",
      "  0.67113381 0.63960296 0.59635223 0.55545465 0.49679988 0.44825053\n",
      "  0.38687855 0.32784544]\n",
      " [0.98290008 0.98038682 0.96807597 0.95490555 0.91131592 0.89056919\n",
      "  0.84395997 0.80281775 0.75584575 0.71019073 0.66634124 0.63005265\n",
      "  0.55895201 0.52135891 0.47499959 0.42546014 0.36732629 0.31499884\n",
      "  0.26162566 0.20585603]\n",
      " [0.95362874 0.95955846 0.93153635 0.91840174 0.85490286 0.82905842\n",
      "  0.77105779 0.71479947 0.65398734 0.59281147 0.57384695 0.5454044\n",
      "  0.45993754 0.41625797 0.36155667 0.30841988 0.2544614  0.19523159\n",
      "  0.14993267 0.09924053]\n",
      " [0.93282075 0.94421681 0.9054313  0.89162301 0.81758883 0.7882008\n",
      "  0.72412362 0.65688868 0.58563834 0.51713921 0.51384583 0.49050735\n",
      "  0.3970616  0.34918232 0.28704104 0.23306613 0.18295682 0.1187307\n",
      "  0.07900398 0.03188776]\n",
      " [1.01720597 1.0057588  1.0101784  0.99493251 0.98637731 0.9750839\n",
      "  0.94824443 0.92142818 0.89015201 0.87401552 0.79653697 0.74674212\n",
      "  0.70171774 0.67168502 0.62788075 0.59038462 0.53260545 0.48391391\n",
      "  0.42058061 0.361417  ]\n",
      " [1.01432055 1.00346119 1.00669786 0.99174035 0.97950642 0.96707238\n",
      "  0.93816499 0.91045725 0.87783986 0.85853104 0.78409128 0.73580484\n",
      "  0.68793338 0.65723349 0.61375985 0.57466928 0.51643438 0.4678813\n",
      "  0.40542032 0.3462662 ]\n",
      " [1.03578694 1.02257164 1.03196355 1.01408255 1.03503957 1.03475398\n",
      "  1.02454341 1.00027817 0.97815956 0.98785859 0.88954331 0.82638995\n",
      "  0.80540983 0.7799919  0.72903957 0.70710087 0.65637428 0.60204944\n",
      "  0.53275848 0.47664631]]\n",
      "train:\n",
      "[[1.         1.         0.75850738 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         0.27794963\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         0.29881577 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         0.12201146 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         0.79079085 0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         0.37684023 0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         0.32232805 0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         0.72645879 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         0.88900578]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  0.03039146 0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         0.64171294 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         0.12384509]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.        ]\n",
      " [1.         1.         1.         1.         1.         0.395474\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  0.28175862 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         0.69105545 0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  0.569838   0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         0.33694665\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  0.97499859 0.        ]\n",
      " [1.         1.         0.58944519 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  0.33053278 0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         0.44144534]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         0.4057744  0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         0.46234152]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         0.2147121  0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         0.62433829 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         0.14556071 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         0.09492688 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         0.59949656 0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         0.62762822 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         0.56373616\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.        ]\n",
      " [1.         0.62095064 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         0.24420437\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         0.75814774 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         0.45929909\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         0.22578387 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         0.56291355]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         0.95387145 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         0.57843062\n",
      "  0.         0.        ]\n",
      " [1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         0.20474767 0.         0.         0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# See how well the training data matches.\n",
    "predictions_train = mlp.predict(X_train)\n",
    "print(\"input:\")\n",
    "print(X_train[0:50,0])\n",
    "print(\"pred:\")\n",
    "print(predictions_train[0:50,:])\n",
    "print(\"train:\")\n",
    "print(y_train[0:50,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to get the resultant probability from a probability confidence array.\n",
    "\n",
    "def get_probability(prob_array):\n",
    "    binSize = 1./prob_array.size\n",
    "    iProb = prob_array.size - 1\n",
    "    while prob_array[iProb] < 0.5 and iProb > 0:\n",
    "        iProb -= 1\n",
    "    prob = binSize*(iProb+prob_array[iProb])\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to compute skill.\n",
    "def compute_skill(predicted,actual):\n",
    "    skill = 0.0\n",
    "    numBins = predicted[0,:].size\n",
    "    numSets = predicted[:,0].size\n",
    "    binSize = 1./numBins\n",
    "    goodExpectedTotal = np.zeros((3,numBins))\n",
    "    for index in range(numSets):\n",
    "        predictedProb = get_probability(predicted[index,:])\n",
    "        actualProb = get_probability(actual[index,:])\n",
    "        iBin = int(predictedProb/binSize+0.5)\n",
    "        if (iBin > numBins - 1):\n",
    "            iBin = numBins - 1\n",
    "        if ((predictedProb > 0.5 and actualProb > 0.5) or (predictedProb < 0.5 and actualProb < 0.5) or (predictedProb == 0.5 and actualProb == 0.5)):\n",
    "            goodExpectedTotal[0,iBin] += 1\n",
    "        goodExpectedTotal[1,iBin] += predictedProb\n",
    "        goodExpectedTotal[2,iBin] += 1\n",
    "    return goodExpectedTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print skill histogram.\n",
    "def print_histogram(goodExpectedTotal):\n",
    "    numBins = goodExpectedTotal[0,:].size\n",
    "    dProb = 1./numBins\n",
    "    headerString   = \"                 \"\n",
    "    totalString    = \"total games:     \"\n",
    "    rightString    = \"number right:    \"\n",
    "    expectedString = \"number expected: \"\n",
    "    rightExpString = \"right/expected:  \"\n",
    "    totalGames = 0\n",
    "    totalGood = 0\n",
    "    totalExpected = 0\n",
    "    for iBin in range(int(0.5*numBins-1.5),numBins):\n",
    "        iBegPct = int(100*iBin*dProb)\n",
    "        iEndPct = iBegPct + int(100*dProb-1)\n",
    "        headerString += ' {0:02d}'.format(iBegPct)\n",
    "        headerString += '-{0:02d}%'.format(iEndPct)\n",
    "        totalString += ' {:6d}'.format(int(goodExpectedTotal[2,iBin]+0.5))\n",
    "        rightString += ' {:6d}'.format(int(goodExpectedTotal[0,iBin]+0.5))\n",
    "        expectedString += ' {:6.1f}'.format(goodExpectedTotal[1,iBin])\n",
    "        if (goodExpectedTotal[1,iBin] > 0.0):\n",
    "            rightExpString += ' {:6.2f}'.format(goodExpectedTotal[0,iBin]/goodExpectedTotal[1,iBin])\n",
    "        else:\n",
    "            rightExpString += ' {:6.2f}'.format(0.0)\n",
    "        totalGames += goodExpectedTotal[2,iBin]\n",
    "        totalGood += goodExpectedTotal[0,iBin]\n",
    "        totalExpected += goodExpectedTotal[1,iBin]\n",
    "    headerString += \"    all\"\n",
    "    totalString += ' {:6d}'.format(int(totalGames+0.5))\n",
    "    rightString += ' {:6d}'.format(int(totalGood+0.5))\n",
    "    expectedString += ' {:6.1f}'.format(totalExpected)\n",
    "    if (totalExpected == 0.0):\n",
    "        totalExpected = 1.0;\n",
    "    rightExpString += ' {:6.2f}'.format(totalGood/totalExpected)\n",
    "    print(headerString)\n",
    "    print(totalString)\n",
    "    print(rightString)\n",
    "    print(expectedString)\n",
    "    print(rightExpString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "                  40-44% 45-49% 50-54% 55-59% 60-64% 65-69% 70-74% 75-79% 80-84% 85-89% 90-94% 95-99%    all\n",
      "total games:           0      0      0     82    473    202    210    219    301    210    320    483   2500\n",
      "number right:          0      0      0     44    252    116    124    156    234    175    272    449   1822\n",
      "number expected:     0.0    0.0    0.0   43.1  272.9  126.5  142.0  159.0  233.7  173.5  280.5  456.7 1887.9\n",
      "right/expected:     0.00   0.00   0.00   1.02   0.92   0.92   0.87   0.98   1.00   1.01   0.97   0.98   0.97\n"
     ]
    }
   ],
   "source": [
    "trainGoodExpectedTotal = compute_skill(predictions_train,y_train)\n",
    "print(\"Training data:\")\n",
    "print_histogram(trainGoodExpectedTotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data:\n",
      "                  40-44% 45-49% 50-54% 55-59% 60-64% 65-69% 70-74% 75-79% 80-84% 85-89% 90-94% 95-99%    all\n",
      "total games:           0      0      0    117    436    206    223    233    280    230    322    453   2500\n",
      "number right:          0      0      0     62    239    123    145    169    207    180    281    423   1829\n",
      "number expected:     0.0    0.0    0.0   61.5  251.6  129.0  150.8  169.2  217.4  190.0  282.3  428.6 1880.4\n",
      "right/expected:     0.00   0.00   0.00   1.01   0.95   0.95   0.96   1.00   0.95   0.95   1.00   0.99   0.97\n"
     ]
    }
   ],
   "source": [
    "# See how well the validation data matches.\n",
    "predictions_valid = mlp.predict(X_valid)\n",
    "validGoodExpectedTotal = compute_skill(predictions_valid,y_valid)\n",
    "print(\"Validation data:\")\n",
    "print_histogram(validGoodExpectedTotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
